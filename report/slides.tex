\documentclass[10pt]{beamer}

\usepackage{amsmath}
\usepackage{amsfonts}
\newcommand{\E}{{\mathbb E}}
\usepackage{longtable}
\newcommand{\Pprob}{\text{I\kern-0.15em P}}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{longtable,booktabs}
\usepackage{threeparttablex, caption}
\usepackage{makecell}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\usepackage[
backend=biber,
style=numeric,
sorting=ynt
]{biblatex}
\addbibresource{references.bib}



\graphicspath{ {./images/} }


\mode<presentation> {

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------


\title{AML in depth study: Reinforcement Learning}


\author{
 Gianluca Giudice
}

\institute[UNIMIB] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
University of Milano-Bicocca\\
\medskip
\textit{g.giudice2@campus.unimib.it}
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your 
\end{frame}

\section{Introduction of Reinforcement learning}

\begin{frame}
\frametitle{Characteristics of Reinforcement Learning}

What makes reinforcement learning different from other machine learning paradigms?
\begin{itemize}
\item There is no supervisor, only a reward signal
\begin{itemize}
	\item Interactions between agent and environment
\end{itemize}
\item Feedback is delayed, not instantaneous
\item Time really matters (sequential, non i.i.d data)
\item Agent's actions affect the subsequent data it receives
\end{itemize}

\end{frame}

%------------------------------------------------
\section{Key concepts in RL}

\begin{frame}
\frametitle{Key concepts in RL}

\tableofcontents[ 
currentsubsection, 
hideothersubsections, 
sectionstyle=show/shaded,
]

\end{frame}

%------------------------------------------------
\subsection{Agent and Environment}

\begin{frame}
\frametitle{Agent and Environment}


\begin{multicols}{2}
	
	\begin{figure}
		\centering
		\includegraphics[width=6cm]{rl-loop.png}
		\caption{Typical RL scenario. \cite{10.5555/3312046}}
		\label{fig:rl-loop}
	\end{figure}
	
	\columnbreak

	\begin{itemize}
		\item At each step $t$ the agent:
		\begin{itemize}
			\item Executes action $A_t$
			\item Receives observation $O_t$
			\item Receives scalar reward $R_t$
		\end{itemize}
	
		\item The environment:
		\begin{itemize}
			\item Receives action $A_t$
			\item Emits observation $O_{t+1}$
			\item Emits scalar reward $R_{t+1}$
		\end{itemize}
	\end{itemize}

\end{multicols}


\end{frame}

%------------------------------------------------
\subsection{States and observations}

\begin{frame}
\frametitle{States and observations}

\begin{itemize}

\item A \textbf{state} $s$ is a complete description of the state of the world
\item An \textbf{observation} $o$ is a partial description of a state
\item Markov decision processes are a mathematical way to describe RL problems. 

\end{itemize}

\begin{definition}
	A \textbf{MDP} is a tuple $(S,A, P, R, \gamma)$, where:
	\begin{itemize}
		\item $S$ is a (finite) set of Markov states $s \in S$
		\item $A$ is a (finite) set of actions $a \in A$
		\item $P$ is dynamics/transition model for each action, that specifies
		\begin{equation}
			P(s_{t+1} =s'|s_t =s,a_t =a)	
		\end{equation}
		\item $R$ is a reward function
		\begin{equation}
			R(s_t =s,a_t =a)=\E[r_t|s_t =s,a_t =a]	
		\end{equation}
		
		\item $\gamma$ is a discount factor $\gamma \in [0,1]$
	\end{itemize}
\end{definition}

\end{frame}

%------------------------------------------------

\subsection{Action spaces}

\begin{frame}
	\frametitle{Action spaces}
	\begin{itemize}
		\item Different environments allow different kinds of actions
		\item The set of all valid actions in a given environment is called the \textbf{action space}
		\item Depending on the environment, the action spaces could be \textbf{discrete} or \textbf{continuous}.
	\end{itemize}
\end{frame}

%------------------------------------------------

\subsection{Policies}
\begin{frame}
	\frametitle{Policies}
\begin{itemize}
	\item A \textbf{policy} defines the learning agent's way of behaving, it is a mapping from perceived states of the environment (i.e. observations) to actions to be taken when in those states.
	\item A policy fully defines the behavior of an agent.
\end{itemize}

\begin{definition}
	 A \textbf{policy} $\pi$ is a distribution over actions given states
	
	\begin{equation}
	\pi(a | s) = \Pprob[A_t=a|S_t=s]
	\end{equation}

\end{definition}

The MDP and agent's policy together give rise to a \textbf{trajectory} $\tau$, which is a sequence of states and actions in the world:
$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1...)$$

\end{frame}

%------------------------------------------------


\subsection{Reward and return}
\begin{frame}

	\frametitle{Reward and return}
	\begin{itemize}
		\item The reward function $R$ depends on the current state of the world and action just taken:
		$$r_t = R(s_t, a_t)$$
		\item The agent aim to maximize the cumulative reward over a trajectory.
		\item The sum of all rewards obtained by the agent is discounted by how far off in the future they are obtained. \item The discount factor is $\gamma \in [0,1]$.
		\item The cumulative reward over a trajectory is:		
			\begin{equation}
			R(\tau) = \sum_{t=0}^{H-1} \gamma^t r_t
			\end{equation}
	\end{itemize}
	
\end{frame}


%------------------------------------------------

\subsection{RL optimization problem}

\begin{frame}

	\frametitle{RL optimization problem}

		\begin{itemize}
			\item The goal in RL is to \textbf{maximize the expected return} when an agent acts according to a policy.
			\item The policy influences the probability distribution over trajectories.
		\item The probability of a $T$-step trajectory is:

			\begin{equation}
				P(\tau|\pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t)
			\end{equation}
		
		
		\item The expected return, denoted by $J(\pi)$, is then:

			\begin{equation}
			J(\pi) = \E_{\tau\sim \pi}{[R(\tau)]}
			\label{eq:loss}
			\end{equation}

		\item The central optimization problem in RL can then be expressed by:
			\begin{equation}
			\pi^* = \arg \max_{\pi} J(\pi)
			\label{eq:max_loss}
			\end{equation}
				with $\pi^*$ being the optimal policy.
		\end{itemize}
		
\end{frame}

%------------------------------------------------

\subsection{Value function}

\begin{frame}

	\frametitle{Value function}

		A crucial aspect of RL are state-value function and action-value function.

		\begin{definition}
			The \textbf{State-Value Function} of a policy, $V^{\pi}(s)$ is the expected return by starting in state $s$ and always acting according to policy $\pi$:
			\begin{equation}
				V^{\pi}(s) = \E_{\tau \sim \pi} [R(\tau)| s_0 = s]
			\end{equation}
		\end{definition}

		\begin{definition}
			The \textbf{Action-Value Function} of a policy, $Q^{\pi}(s,a)$ is the expected return by starting in state $s$ and taking an arbitrary action $a$ (which may not have come from the policy), and then forever after acting according to the policy $\pi$:
			\begin{equation}
					Q^{\pi}(s,a) = \E_{\tau \sim \pi}[R(\tau)| s_0 = s, a_0 = a]	
			\end{equation}
			
		\end{definition}
	
\end{frame}

%------------------------------------------------

\subsection{Optimal value function and optimal policy }


\begin{frame}
	\frametitle{Optimal value function and optimal policy }
	Value functions define a partial ordering over policies,
	$\pi \geq \pi'$ if and only if $v_\pi(s) \geq v_{\pi'}(s)$ for all $s \in S$. There is always at least one policy that is better than or equal to all other policies, this is the optimal policy $\pi^*$.

	\begin{definition}
		The \textbf{Optimal State-Value Function}, $V^*(s)$ is the expected return by starting in state $s$ and always acting according to the \textit{optimal} policy in the environment:
		
		\begin{equation}
			V^{*}(s) = \max_\pi \E_{\tau \sim \pi} [R(\tau)| s_0 = s] = \max_\pi V^\pi(s)
		\end{equation}
	\end{definition}

	\begin{definition}
		The \textbf{Optimal Action-Value Function}, $Q^*(s,a)$ is the expected return by starting in state $s$, taking an arbitrary action $a$, and then forever after acting according to the \textit{optimal} policy in the environment:
		
		\begin{equation}
		Q^*(s,a) = \max_{\pi} \E_{\tau \sim \pi}{[R(\tau)| s_0 = s, a_0 = a]} = \max_\pi Q^\pi(s,a)
		\end{equation}
	\end{definition}

\end{frame}


\begin{frame}
	\frametitle{Optimal value function and optimal policy }
	By having $Q^*(s,a)$, it's possible to directly obtain the optimal action $a^*(s)$:

	\begin{equation}
		a^*(s) = \argmax_{a \in A} Q^* (s,a)
	\end{equation}

	Therefore an optimal policy can be found by maximizing over $Q^*(s,a)$:

	\begin{equation}
		\pi^*(a|s) = 
		\begin{cases}
			1 & if \; a = \argmax_{a \in A} Q^*(s,a)\\
			0 & otherwise
		\end{cases}
		\label{eq:opt-policy}
	\end{equation}

\end{frame}

%------------------------------------------------

\section{RL algorithms}


\begin{frame}
	\frametitle{Taxonomy of RL algorithms}
	The final objective of RL is to find the best policy, i.e.:

	\begin{equation}
		\pi^* = \argmax_\pi \E_{\tau \sim \pi}{[R(\tau)]}
		\label{eq:objective}
	\end{equation}
	

	There are a variety of algorithms to do so, depending on what to learn and how to learn it.

	\begin{figure}[h]
		\centering
		\includegraphics[width=7cm]{rl-taxonomy.png}
		\caption{A non-exhaustive taxonomy of RL algorithms. \cite{SpinningUp2018}}
		\label{fig:rl-taxonomy}
	\end{figure}

\end{frame}


\begin{frame}
	\frametitle{Q-Learing}
	An example of RL algorithm is Q-learning.\\
	\begin{itemize}
		\item Q-Learning methods learn an approximation $Q_{\theta}(s,a)$ for the optimal action-value function, $Q^*(s,a)$.
		\item The Q-learning algorithm keeps an estimate $Q(s,a)$ of $Q^*(s,a)$ for each state-action pair $(s,a) \in S \times A$.
		\item By observing $(s_t, a_t, r_{t+1}, s_{t+1})$ the estimates are updated.
	\end{itemize}
	
	The idea behind Deep Reinforcement Learning is that since Q-Learning is simply function approximation, a Deep Neural Network could be used to approximate this function. 

\end{frame}

%------------------------------------------------

\section{Case study: AlphaGo Zero}

\begin{frame}
	\frametitle{AlphaGo Zero}
	
	\tableofcontents[ 
	currentsubsection, 
	hideothersubsections, 
	sectionstyle=show/shaded,
	]
	
\end{frame}


\begin{frame}
	\frametitle{Motivation of AlphaGo Zero}
	The game of Go is one of the most challenging classic games for artificial intelligence due to its enormous search space and the difficulty of evaluating board positions and moves.\\

	Search algorithms that use minimax, alpha-beta pruning and evaluation function (the techniques used in DeepBlue\cite{CAMPBELL200257} to beat Garry Kasparov in the game of chess) can not be used in the game of Go due to its huge complexity.

	\begin{table}

		\tiny
		\centering
		
		\begin{tabular}{|cccccc|}

		\hline
		Game  &
		Board size (positions)  &
		Game-tree complexity (log to base 10)  &
		Complexity class\\
		\hline
		\hline
		Tic-tac-toe	& 9  & 5 & PSPACE-complete\cite{Reisch1981HexIP} \\
		Connect Four & 42 & 21 & in PSPACE \cite{Allis1994SearchingFS} \\
		Checkers & 32  & 40   & EXPTIME-complete \cite{Robson1984NBN} \\
		Chess & 64 & 123  & EXPTIME-complete \cite{FRAENKEL1981199} \\
		Go (19x19) & 361 & 360 & EXPTIME-complete \cite{inproceedings} \\
		\hline
		\end{tabular}
		\caption{Complexity of the most famous board game.}
		\label{tab:game-complexity}
		
	\end{table}

	
	\begin{figure}[h]
		\centering
		\includegraphics[width=3cm]{go-board.jpg}
	\end{figure}


\end{frame}

%------------------------------------------------

\begin{frame}
	\frametitle{AlphaGo Zero}
	Characteristics of AlphaGo Zero:
	\begin{itemize}
		\item AlphaGo Zero \cite{silver2017mastering}, is an algorithm based on reinforcement learning.
		\item It started from zero, without human knowledge beyond game rules.
		\item AlphaGo Zero achieved superhuman performance in Go.
	\end{itemize}

	A neural network is used for value-function approximation:
	\begin{itemize}
		\item In AlphaGo Zero a deep neural network $f_\theta$ with parameters $\theta$ is used.
		\item The neural network takes as input the raw board representation $s$.
		\item The first layer of the neural network is a convolutional layer, followed by 40 convolutional blocks with skip-connections.
		\item The neural network has two outputs (multitask learning):
		\begin{itemize}		
			\item Vector $p$: the vector of move probabilities $p$ represents the probability of selecting each move, $p_a = P(a | s)$
			\item Scalar $v$: the value $v$ is a scalar evaluation, estimating the probability of the current player winning from position $s$
		\end{itemize}
	\end{itemize}
\end{frame}


%------------------------------------------------

\begin{frame}
	\frametitle{Self-play in AlphaGo Zero}
	
	\begin{itemize}
		\item The neural network in AlphaGo Zero is trained from games of self-play.
		\item In each position $s$, a MCTS search is executed, guided by the neural network $f_\theta$.
		\item The MCTS search outputs probabilities $\pi$ of playing each move
		\item MCTS may be viewed as a policy improvement operator, since these search probabilities select much stronger moves than the raw move probabilities $p$ of the neural network $f_\theta(s)$.
		\item During the self-play the MCTS-based policy is used to select each move, and the game winner $z$ is used as a sample of the value $v$ (the winning player).
		\item Parameters are updated to make the move probabilities and value $f_\theta(s) = (p, v) $ match the search probabilities and self play winner $(\pi, z)$.
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Self-play in AlphaGo Zero}
		
	\begin{figure}[H]
		\centering
		\includegraphics[width=7cm]{alpha_go_zero_training.png}
		
		\caption{
			\textbf{Self-play} and \textbf{training} AlphaGo Zero.\cite{Silver_2016}
			}
		\label{fig:alpha-zero-self_play}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Self-play in AlphaGo Zero}
	\begin{itemize}
		\item The MCTS uses the neural network $f_\theta$ to guide its simulations.
		$$f_\theta(s) = (P(s, \cdot), \, V(s))$$
		\item Each edge $(s, a)$ in the search tree stores a prior probability $P(s, a) = p_\theta(a | s)$, a visit count $N(s, a)$, and an action-value $Q(s, a)$.
	
	\item Each simulation starts from the root state and selects moves:
	\begin{equation}
		a_t  = \argmax_a{Q(s_t,a) + U(s_t,a)}
	\end{equation}

	\begin{equation}
		Q(s, a) = \frac{1}{N(s,a)} \sum_{s'\, | \, s,\, a \rightarrow s'}{V(s')},
	\end{equation}

	\begin{equation}
		U(s, a) = c_{puct}P(s, a)\frac{\sum_b N(s,b)}{1+N(s,a)}
	\end{equation}

	\item $U(s_t,a)$ acts as an exploration factor governed by $c_{puct}$.
	\item This strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action value.
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Self-play in AlphaGo Zero}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=10cm]{alpha-zero-mcts.png}
		\caption{\textbf{MCTS procedure} in AlphaGo Zero.\cite{Silver_2016}}
		\label{fig:alpha-zero-mcts}
	\end{figure}
	
	The parameters $\theta$ are adjusted by gradient descent on the loss function $L$:

	\begin{equation}
		f_\theta(s) = (p,v),\quad L = (z-v)^2 - \pi_t log(p) + c \norm{\theta}^2
	\end{equation}

\end{frame}


\begin{frame}
	\frametitle{Empirical results of AlphaGo Zero}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=11cm,trim={0px 0px 0px 45px},clip]{alpha-go-zero_empirical_results.png}
		\caption{AlphaGo Zero performance. \cite{Silver_2016}}
	\end{figure}

\end{frame}


\section{Hands-On: Connect4 Zero}

\begin{frame}
	\frametitle{Hands-On: Connect4 Zero}
	
	\tableofcontents[ 
	currentsubsection, 
	hideothersubsections, 
	sectionstyle=show/shaded,
	]
	
\end{frame}

\begin{frame}
	\frametitle{Connect4 Zero}

	\begin{itemize}
		\item Use the techniques of AlphaGo Zero \cite{silver2017mastering} to achieve master level in Connect4.
		\item I considered the game of Connect4 due to the limited computational resources at my disposal.
		\item Starting from this code\footnote{\url{https://github.com/suragnair/alpha-zero-general}}, I changed it to support the game of Connect4. I also added:
		\begin{itemize}
			\item A graphical user interface to play against the agent.
			\item A simpler neural network architecture.
			\item Episodes of self-play are performed in parallel to drastically reduce the training time (taking inspiration form the work of David Silver "Playing Atari with Deep Reinforcement Learning" \cite{mnih2013atari}).

		\end{itemize}

	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Connect4 Zero}
	\Huge{\centerline{Live Demo}}

\end{frame}

%------------------------------------------------

\end{document} 