@article{silver2017mastering,
	title={Mastering the game of go without human knowledge},
	author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	journal={nature},
	volume={550},
	number={7676},
	pages={354--359},
	year={2017},
	publisher={Nature Publishing Group}
}

@book{10.5555/3312046,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Reinforcement Learning: An Introduction},
	year = {2018},
	isbn = {0262039249},
	publisher = {A Bradford Book},
	address = {Cambridge, MA, USA},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}

@book{russell2016artificial,
	title={Artificial Intelligence: a modern approach},
	author={Russell, Stuart J. and Norvig, Peter},
	edition={3},
	year={2009},
	publisher={Pearson}
}

@article{SpinningUp2018,
	author = {Achiam, Joshua},
	title = {Spinning Up in Deep Reinforcement Learning},
	year = {2018}
}

@Misc{silver2015,author = {David Silver},title = {Lectures on 
	Reinforcement Learning},howpublished = {\textsc{url:}~\url
	{https://www.davidsilver.uk/teaching/}},year = {2015}}

@book{series/synthesis/2010Szepesvari,
	added-at = {2010-09-04T00:00:00.000+0200},
	author = {Szepesvári, Csaba},
	biburl = {https://www.bibsonomy.org/bibtex/2a2877247f9323bac6f51f2ccc231efb5/dblp},
	booktitle = {Algorithms for Reinforcement Learning},
	ee = {http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009},
	interhash = {46c07bb11e2b5fe91663d461efbc2af1},
	intrahash = {a2877247f9323bac6f51f2ccc231efb5},
	keywords = {dblp},
	series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	timestamp = {2010-09-07T11:31:49.000+0200},
	title = {Algorithms for Reinforcement Learning},
	url = {http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009},
	year = 2010
}

@article{mnih2013atari,
	abstract = {We present the first deep learning model to successfully learn control
	policies directly from high-dimensional sensory input using reinforcement
	learning. The model is a convolutional neural network, trained with a variant
	of Q-learning, whose input is raw pixels and whose output is a value function
	estimating future rewards. We apply our method to seven Atari 2600 games from
	the Arcade Learning Environment, with no adjustment of the architecture or
	learning algorithm. We find that it outperforms all previous approaches on six
	of the games and surpasses a human expert on three of them.},
	added-at = {2019-07-12T20:11:01.000+0200},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
	keywords = {},
	note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
	timestamp = {2019-07-12T20:11:01.000+0200},
	title = {Playing Atari with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1312.5602},
	year = 2013
}

@article{Silver_2016,
	added-at = {2016-03-11T14:36:05.000+0100},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	biburl = {https://www.bibsonomy.org/bibtex/29e987f58d895c490144693139cbc90c7/ytyoun},
	doi = {10.1038/nature16961},
	interhash = {48430c7891aaf9fe2582faa8f5d076c1},
	intrahash = {9e987f58d895c490144693139cbc90c7},
	journal = {Nature},
	keywords = {baduk go google},
	month = jan,
	number = 7587,
	pages = {484--489},
	publisher = {Nature Publishing Group},
	timestamp = {2016-03-11T14:37:40.000+0100},
	title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
	volume = 529,
	year = 2016
}

@inproceedings{Allis1994SearchingFS,
	title={Searching for solutions in games and artificial intelligence},
	author={L. Victor Allis},
	year={1994}
}

@article{Robson1984NBN,
	title={N by N Checkers is Exptime Complete},
	author={John Michael Robson},
	journal={SIAM J. Comput.},
	year={1984},
	volume={13},
	pages={252-267}
}

@article{FRAENKEL1981199,
	title = {Computing a perfect strategy for n × n chess requires time exponential in n},
	journal = {Journal of Combinatorial Theory, Series A},
	volume = {31},
	number = {2},
	pages = {199-214},
	year = {1981},
	issn = {0097-3165},
	doi = {https://doi.org/10.1016/0097-3165(81)90016-9},
	url = {https://www.sciencedirect.com/science/article/pii/0097316581900169},
	author = {Aviezri S Fraenkel and David Lichtenstein},
	abstract = {It is proved that a natural generalization of chess to an n × n board is complete in exponential time. This implies that there exist chess positions on an n × n chessboard for which the problem of determing who can win from that position requires an amount of time which is at least exponential in n.}
}


@article{doi:10.1080/14786445008521796,
	author = { Claude E.   Shannon },
	title = {XXII. Programming a computer for playing chess},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	volume = {41},
	number = {314},
	pages = {256-275},
	year  = {1950},
	publisher = {Taylor & Francis},
	doi = {10.1080/14786445008521796},
	
	URL = { 
	https://doi.org/10.1080/14786445008521796
	
	},
	eprint = { 
	https://doi.org/10.1080/14786445008521796
	
	}
	
}


@article{IIDA2002121,
	title = {Computer shogi},
	journal = {Artificial Intelligence},
	volume = {134},
	number = {1},
	pages = {121-144},
	year = {2002},
	issn = {0004-3702},
	doi = {https://doi.org/10.1016/S0004-3702(01)00157-6},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370201001576},
	author = {Hiroyuki Iida and Makoto Sakuta and Jeff Rollason},
	keywords = {Shogi, Computer shogi, Alpha-beta search, Selective search, Quiescence search, Evaluation function},
	abstract = {This paper describes the current state of the art in computer shogi. Shogi (Japanese chess) promises to be a good vehicle for future research into game-playing programs that are based on tree-searching paradigms. This paper shows where chess and shogi are similar, and details the important areas that make shogi programming of particular interest. A crucial difference is the game-tree complexity, which is significantly higher in shogi than in chess. Three important differences are the “drop” rule, the diverging character of the game, and the slow build-up of forces. They make it difficult to have effective opening and endgame procedures. After a short summary of the rules of shogi and an outline of the main areas of current work in computer shogi, we provide an overview of the history of computer shogi, in which computer-shogi activities both in human tournaments and in exhibition events are given. We conjecture that by the year 2010 a computer will be comparable in strength to the best human players. The most important techniques used in computer shogi are described. We focus on issues such as opening play, selective search, quiescence search, solving tactical exchanges without tree searching, position evaluation and endgame play. At the end the key challenges in computer shogi are enumerated, and finally, concluding remarks are given.}
}

@inproceedings{inproceedings,
	author = {Robson, John},
	year = {1983},
	month = {01},
	pages = {413-417},
	title = {The Complexity of Go.},
	volume = {9},
	journal = {IFIP Congress Series}
}

@InProceedings{10.1007/3-540-48957-6_8,
	author="Buro, Michael",
	editor="van den Herik, H. Jaap
	and Iida, Hiroyuki",
	title="From Simple Features to Sophisticated Evaluation Functions",
	booktitle="Computers and Games",
	year="1999",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="126--145",
	abstract="This paper discusses a practical framework for the semi-automatic construction of evaluation-functions for games. Based on a structured evaluation function representation, a procedure for exploring the feature space is presented that is able to discover new features in a computationally feasible way. Besides the theoretical aspects, related practical issues such as the generation of training positions, feature selection, and weight fitting in large linear systems are discussed. Finally, we present experimental results for Othello, which demonstrate the potential of the described approach.",
	isbn="978-3-540-48957-3"
}

@Inbook{Bouzy2004,
	author="Bouzy, B.
	and Helmstetter, B.",
	editor="Van Den Herik, H. Jaap
	and Iida, Hiroyuki
	and Heinz, Ernst A.",
	title="Monte-Carlo Go Developments",
	bookTitle="Advances in Computer Games: Many Games, Many Challenges",
	year="2004",
	publisher="Springer US",
	address="Boston, MA",
	pages="159--174",
	abstract="We describe two Go programs, Olga and Oleg, developed by a Monte-Carlo approach that is simpler than Bruegmann's (1993) approach. Our method is based on Abramson (1990). We performed experiments, to assess ideas on (1) progressive pruning, (2) all moves as first heuristic, (3) temperature, (4) simulated annealing, and (5) depth-two tree search within the Monte-Carlo framework. Progressive pruning and the all moves as first heuristic are good speed-up enhancements that do not deteriorate the level of the program too much. Then, using a constant temperature is an adequate and simple heuristic that is about as good as simulated annealing. The depth-two heuristic gives deceptive results at the moment. The results of our Monte-Carlo programs against knowledge-based programs on 9x9 boards are promising. Finally, the ever-increasing power of computers lead us to think that Monte-Carlo approaches are worth considering for computer Go in the future.",
	isbn="978-0-387-35706-5",
	doi="10.1007/978-0-387-35706-5_11",
	url="https://doi.org/10.1007/978-0-387-35706-5_11"
}



@article{MULLER2002145,
	title = {Computer Go},
	journal = {Artificial Intelligence},
	volume = {134},
	number = {1},
	pages = {145-179},
	year = {2002},
	issn = {0004-3702},
	doi = {https://doi.org/10.1016/S0004-3702(01)00121-7},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370201001217},
	author = {Martin Müller},
	keywords = {Computer Go, Go programs, Game tree search, Knowledge representation},
	abstract = {Computer Go is one of the biggest challenges faced by game programmers. This survey describes the typical components of a Go program, and discusses knowledge representation, search methods and techniques for solving specific subproblems in this domain. Along with a summary of the development of computer Go in recent years, areas for future research are pointed out.}
}

@article{CAMPBELL200257,
	title = {Deep Blue},
	journal = {Artificial Intelligence},
	volume = {134},
	number = {1},
	pages = {57-83},
	year = {2002},
	issn = {0004-3702},
	doi = {https://doi.org/10.1016/S0004-3702(01)00129-1},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
	author = {Murray Campbell and A.Joseph Hoane and Feng-hsiung Hsu},
	keywords = {Computer chess, Game tree search, Parallel search, Selective search, Search extensions, Evaluation function},
	abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.}
}